{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AmzonTop50BestSellingBooks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOB/oi6Jia9+L0KC6q2EaRw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgadela/ML_HandsOn/blob/master/AmzonTop50BestSellingBooks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzPImjbe1De-",
        "outputId": "5d17147b-aeb6-4b15-8259-4a37277887f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "data=pd.read_csv(\"bestsellers_with_categories.csv\")\n",
        "data.info()\n",
        "\n",
        "#Preprocessing\n",
        "stop_words = stopwords.words('english')\n",
        "def process_name(name):\n",
        "    name = re.sub(r'\\d+', ' ', name)\n",
        "    name = name.split()\n",
        "    name = \" \".join([word for word in name if word not in stop_words])\n",
        "    return name\n",
        "names = data['Name'].apply(process_name)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(names)\n",
        "vocab_length = len(tokenizer.word_index) + 1\n",
        "names = tokenizer.texts_to_sequences(names)\n",
        "max_seq_length = np.max(list(map(lambda name: len(name), names)))\n",
        "names = pad_sequences(names, maxlen=max_seq_length, padding='post')\n",
        "names\n",
        "\n",
        "data=data.drop('Name',axis=1)\n",
        "genre_mapping={'Non Fiction' : 0,'Fiction' : 1}\n",
        "data['Genre']=data['Genre'].replace(genre_mapping)\n",
        "print(\"No of unique authors:\" , len(data['Author'].unique()))\n",
        "\n",
        "def onehot_encode(df, column, prefix):\n",
        "    df = df.copy()\n",
        "    dummies = pd.get_dummies(df[column], prefix=prefix)\n",
        "    df = pd.concat([df, dummies], axis=1)\n",
        "    df = df.drop(column, axis=1)\n",
        "    return df\n",
        "\n",
        "data=onehot_encode(data,'Author','auth')\n",
        "data\n",
        "print(\"data\",data)\n",
        "\n",
        "#splitting/scaling\n",
        "y=data['Genre'].copy()\n",
        "X=data.drop('Genre',axis=1).copy()\n",
        "scaler=StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "names_train, names_test, X_train, X_test, y_train, y_test = train_test_split(names, X, y, train_size=0.7, random_state=100)\n",
        "\n",
        "#Model/Training\n",
        "embedding_dim = 64\n",
        "# Name features\n",
        "name_input = tf.keras.Input(shape=(20,), name=\"name_input\")\n",
        "embedding = tf.keras.layers.Embedding(\n",
        "    input_dim=vocab_length,\n",
        "    output_dim=embedding_dim,\n",
        "    input_length=max_seq_length,\n",
        "    name=\"name_embedding\"\n",
        ")(name_input)\n",
        "\n",
        "name_flatten = tf.keras.layers.Flatten(name=\"name_flatten\")(embedding)\n",
        "# Other features\n",
        "other_input = tf.keras.Input(shape=(252,), name=\"other_input\")\n",
        "hidden_1 = tf.keras.layers.Dense(256, activation='relu', name=\"other_dense_1\")(other_input)\n",
        "hidden_2 = tf.keras.layers.Dense(256, activation='relu', name=\"other_dense_2\")(hidden_1)\n",
        "# Concatenate and output\n",
        "concat = tf.keras.layers.concatenate([name_flatten, hidden_2], name=\"concatenate\")\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(concat)\n",
        "model = tf.keras.Model(inputs=[name_input, other_input], outputs=outputs)\n",
        "print(model.summary())\n",
        "tf.keras.utils.plot_model(model)\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.AUC(name='auc')\n",
        "    ]\n",
        ")\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "history = model.fit(\n",
        "    [names_train, X_train],\n",
        "    y_train,\n",
        "    validation_split=0.12,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=3,\n",
        "            restore_best_weights=True) ]\n",
        ")\n",
        "model.evaluate([names_test, X_test], y_test)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 550 entries, 0 to 549\n",
            "Data columns (total 7 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   Name         550 non-null    object \n",
            " 1   Author       550 non-null    object \n",
            " 2   User Rating  550 non-null    float64\n",
            " 3   Reviews      550 non-null    int64  \n",
            " 4   Price        550 non-null    int64  \n",
            " 5   Year         550 non-null    int64  \n",
            " 6   Genre        550 non-null    object \n",
            "dtypes: float64(1), int64(3), object(3)\n",
            "memory usage: 30.2+ KB\n",
            "No of unique authors: 248\n",
            "data      User Rating  Reviews  ...  auth_Wizards RPG Team  auth_Zhi Gang Sha\n",
            "0            4.7    17350  ...                      0                  0\n",
            "1            4.6     2052  ...                      0                  0\n",
            "2            4.7    18979  ...                      0                  0\n",
            "3            4.7    21424  ...                      0                  0\n",
            "4            4.8     7665  ...                      0                  0\n",
            "..           ...      ...  ...                    ...                ...\n",
            "545          4.9     9413  ...                      0                  0\n",
            "546          4.7    14331  ...                      0                  0\n",
            "547          4.7    14331  ...                      0                  0\n",
            "548          4.7    14331  ...                      0                  0\n",
            "549          4.7    14331  ...                      0                  0\n",
            "\n",
            "[550 rows x 253 columns]\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "name_input (InputLayer)         [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "other_input (InputLayer)        [(None, 252)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "name_embedding (Embedding)      (None, 20, 64)       74240       name_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "other_dense_1 (Dense)           (None, 256)          64768       other_input[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "name_flatten (Flatten)          (None, 1280)         0           name_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "other_dense_2 (Dense)           (None, 256)          65792       other_dense_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 1536)         0           name_flatten[0][0]               \n",
            "                                                                 other_dense_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 1)            1537        concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 206,337\n",
            "Trainable params: 206,337\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 0.6518 - accuracy: 0.6391 - auc: 0.6892 - val_loss: 0.5858 - val_accuracy: 0.7447 - val_auc: 0.8064\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4300 - accuracy: 0.9438 - auc: 0.9911 - val_loss: 0.4968 - val_accuracy: 0.8298 - val_auc: 0.8703\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2393 - accuracy: 0.9852 - auc: 0.9992 - val_loss: 0.4099 - val_accuracy: 0.7872 - val_auc: 0.9032\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0908 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.3568 - val_accuracy: 0.8298 - val_auc: 0.9126\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0278 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.3326 - val_accuracy: 0.8298 - val_auc: 0.9239\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0094 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.3406 - val_accuracy: 0.8298 - val_auc: 0.9295\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0044 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.3479 - val_accuracy: 0.8298 - val_auc: 0.9286\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0027 - accuracy: 1.0000 - auc: 1.0000 - val_loss: 0.3533 - val_accuracy: 0.8298 - val_auc: 0.9295\n",
            "6/6 [==============================] - 0s 2ms/step - loss: 0.3125 - accuracy: 0.8727 - auc: 0.9394\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.31249886751174927, 0.8727272748947144, 0.9393571615219116]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}